## Replication Package


Scaling Up: Revisiting Mining Android Sandboxes at Scale for Malware Classification

### Abstract

The widespread use of smartphones in daily life has raised concerns about privacy and security among researchers and practitioners. Privacy issues are generally highly prevalent in mobile applications, particularly targeting the Android platformâ€”the most popular mobile operating system. For this reason, several techniques have been proposed to identify malicious behavior in Android applications, including the Mining Android Sandbox approach (MAS approach), which aims to identify malicious behavior in repackaged Android applications (apps). However, previous empirical studies that evaluate the MAS approach has been evaluated in small datasets, typically consisting of only 102 pairs of original and repackaged apps. This limitation raises questions about the external validity of their findings and whether the MAS approach can be generalized to larger datasets. To address these concerns, this paper presents the results of a replication study focused on evaluating the performance of the MAS approach regarding its capabilities of correctly classifying malware from different families. Unlike previous studies, our research employs a dataset that is an order of magnitude larger, comprising 4,076 pairs of apps covering a more diverse range of Android malware families. Surprisingly, our findings indicate a poor performance of the MAS approach for identifying malware, with the F1-score decreasing from 0.89 for the small dataset used in the previous studies to 0.54 in our more extensive dataset. Upon closer examination, we discovered that certain malware families partially account for the low accuracy of the MAS approach, which fails to classify a repackaged version of an app as malware correctly. Our findings highlight the limitations of the MAS approach, particularly when scaled, and underscore the importance of complementing it with other techniques to detect a broader range of malware effectively. This opens avenues for further discussion on addressing the blind spots that affect the accuracy of the MAS approach.

### Malware Dataset

We use a curated dataset of 4,076 repackaged apps based on two repositories, RePack (https://github.com/serval-snt-uni-lu/RepackageRepo.git) and AndroMalPack (https://github.com/hasnainrafique/AndroMalPack-Dataset). Both were curated using automatic procedures that extract repackaged apps from the [Androzoo repository] (https://androzoo.uni.lu/gp-metadata), and arrange the samples on the following CSV [file](https://github.com/droidxp/paper-ecoop-results/blob/main/Samples.csv). In this file, the columns are: First - original app hash, Second - repackage app hash. The original dataset from previous research works has 102 repackaged apps, which we also separate and available in the following CSV [file](https://github.com/droidxp/paper-ecoop-results/blob/main/originalSamples.csv). To download both dataset we used this python [script](https://github.com/droidxp/paper-ecoop-results/blob/main/getApps.py)

We queried the VirusTotal repository (https://www.virustotal.com/gui/home/upload) to find out which repackaged apps in our dataset have been indeed labeled as malware, and if positive, find which malware family the sample came from. To collect this information, we use avclass2 tool (https://github.com/malicialab/avclass). The first step for that is to create a hash [list](https://github.com/droidxp/paper-ecoop-results/blob/main/listRepackagedHash.csv) of all repackage app hash which we would like to check at VirusTotal. With this list, we use a python [script](https://github.com/droidxp/paper-ecoop-results/blob/main/urltoFile.py) to download all Json files from VirusTotal, which we use at avclass2 tool to get information about repackage family. After this procedure, we get the following [dataset](https://github.com/droidxp/paper-ecoop-results/blob/main/avClassResultRepackaged.csv). In this dataset, if the sample was flagged by more than 1 AV engine, the column 'family' contains the family name, or 'None' if VirusTotal do not detected the malware family. If the sample was flagged by just 1 or 0 AV engine, the column contains 'None'.

We also characterize our dataset according to the similarity between the original and repackage app versions, using SimiDroid tool (https://github.com/lilicoding/SimiDroid). As a first step, we use SimiDroid tool to get Json files containing information about methods identical, similar, new, deleted, and similarity Score from our sample. As a final result, we have this CSV [file](https://github.com/droidxp/paper-ecoop-results/blob/main/summarySimiDroid.csv) with information about differences between app pairs (original/repackage), and similarity score from our samples.

### Data Collection

We take advantage of the [DroidXP](https://github.com/droidxp/benchmark) infrastructure for data collection. With DroidXP we collect all relevant information, such as calls to sensitive APIs, during the test execution performed by the test generator tool Droidbot. We execute DroidXP using Droidbot for 180 seconds, and repeat all processes 3 times for each app pair (original/repackage). The final result from this exploratory step is compiled in this [zip](https://github.com/droidxp/paper-ecoop-results/blob/main/180_preview_work.zip) file. Because of space issues at the repository, this zip file has just the apps explored in the original works (102 app samples).

After the exploratory step, we produce a [dataset](https://github.com/droidxp/paper-droidxptrace-results/blob/main/methods_explored/output/methods_explored.zip) with the sensitive methods filter from this [list](https://github.com/droidxp/paper-ecoop-results/blob/main/methods_explored/scripts/sensitive_methods.txt), that both app versions call during their 3 executions. To create this dataset, we take advantage of 2 pyhton scripts present in our repository. Before executing both scripts, we unzip the result folder from the exploratory step inside methods_explored folder, present at our repository, and execute [run.sh](https://github.com/droidxp/paper-ecoop-results/blob/main/methods_explored/run.sh) command to call both scripts. The [first](https://github.com/droidxp/paper-ecoop-results/blob/main/methods_explored/scripts/generate_union_of_executions.py) python script will generate the union of all sensitive APIs explored from all executions, and the [second](https://github.com/droidxp/paper-ecoop-results/blob/main/methods_explored/scripts/compute_diff_between_benign_and_malign.py) will compute the diff between sensitive APIs called at both versions (original/repackage). This procedure also will generate 2 final CSVs file. The [first](https://github.com/droidxp/paper-ecoop-results/blob/main/methods_explored/output/diffs/summary.csv) is the summary of occurs of different sensitive methods at both app versions, and the [second](https://github.com/droidxp/paper-ecoop-results/blob/main/methods_explored/output/diffs/methods_in_diff.csv) CSV file presents how many times each sensitive method, was inserted when analyzing the whole sample.

To merge all results into just one dataset and facilitate analysis, we generate a [partial](https://github.com/droidxp/paper-ecoop-results/blob/main/sample_final_ds_before_VT_check.csv) dataset (4,737 samples) taking advantage of a R [script](https://github.com/droidxp/paper-ecoop-results/blob/main/merge-datasets.Rmd) file. We get the final dataset (4,076 samples) when apply a filter at [partial](https://github.com/droidxp/paper-ecoop-results/blob/main/sample_final_ds_before_VT_check.csv) dataset, recovering just the benign original app (benign=True). The original dataset from previous research which we replicated is available at the CSV [file](https://github.com/droidxp/paper-ecoop-results/blob/main/small_ds.csv). In the end, we are left with our final dataset, available at CSV [file](https://github.com/droidxp/paper-ecoop-results/blob/main/large_ds.csv) that present the impact of all 116 malware families on the performance of Mining Android Sandbox approach (MAS) approach. All data presented in the article were generated from this final dataset and presented at this [html file](https://github.com/droidxp/paper-ecoop-results/blob/main/analysis.html), taking advantage of this R [script](https://github.com/droidxp/paper-ecoop-results/blob/main/analysis.Rmd).

